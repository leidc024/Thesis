# Context-Aware Baybayin Transliteration Disambiguation

A graph-based approach for disambiguating Baybayin OCR transliterations using contextual embeddings and linguistic features.

## ğŸ¯ Results

| Method | Ambiguous Word Accuracy |
|--------|------------------------|
| MaBaybay Default | ~38% |
| Embedding-Only (WE-Only) | ~66% |
| bAI-bAI WE-Only (reported) | 77.46% |
| **bAI-bAI LLM (reported)** | **90.52%** |
| **Our Graph+Features** | **92.72%** âœ“ |

**Key Achievement:** Our method exceeds the LLM-based approach (+2.2 percentage points) while being significantly faster (no API calls required).

## ğŸ“Š Test Configuration

- **Test Set:** 500 sentences (balanced by ambiguity type)
- **Total Words:** 4,500
- **Ambiguous Words:** 756
- **Clean Evaluation:** Test sentences excluded from corpus statistics

## ğŸ—ï¸ Architecture

```
Input: OCR Candidates â†’ [word1, [cand_a, cand_b], word3, ...]
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE EXTRACTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Semantic Similarity (RoBERTa)         Weight: 0.3       â”‚
â”‚     - Filipino RoBERTa: jcblaise/roberta-tagalog-base       â”‚
â”‚     - Contextual embeddings for candidate vs. sentence      â”‚
â”‚                                                              â”‚
â”‚  2. Corpus Frequency                      Weight: 0.4       â”‚
â”‚     - ~286k words from Filipino text corpora                â”‚
â”‚     - Log-normalized frequency scores                       â”‚
â”‚                                                              â”‚
â”‚  3. Co-occurrence (Bigrams)               Weight: 0.2       â”‚
â”‚     - P(word | prev_word) + P(next_word | word)            â”‚
â”‚     - Laplace smoothing for unseen bigrams                  â”‚
â”‚                                                              â”‚
â”‚  4. Morphological Features                Weight: 0.1       â”‚
â”‚     - Filipino prefix/suffix patterns                       â”‚
â”‚     - Reduplication detection                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
           Combined Score = Î£(weight_i Ã— feature_i)
                              â†“
              Select candidate with highest score
                              â†“
Output: Disambiguated sentence â†’ [word1, cand_a, word3, ...]
```

## ğŸ”¤ Ambiguity Types Handled

| Baybayin | Latin Options | Example |
|----------|---------------|---------|
| áœ | E / I | "sila" vs "sela" |
| áœ‚ | O / U | "buto" vs "boto" |
| áœ‡ | D / R | "dito" vs "rito" |

## ğŸ“ Project Structure

```
Thesis/
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ disambiguator.py          # Main disambiguation model
â”‚   â”œâ”€â”€ corpus.py                 # Corpus statistics module
â”‚   â”œâ”€â”€ morphology.py             # Filipino morphology analyzer
â”‚   â””â”€â”€ baselines.py              # Baseline models for comparison
â”‚
â”œâ”€â”€ evaluate.py                   # Main evaluation script
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ test_sentences_500.txt    # Test sentences
â”‚   â”‚   â””â”€â”€ candidates_results_v2.json # OCR candidates
â”‚   â””â”€â”€ results/
â”‚       â””â”€â”€ evaluation_results.json   # Evaluation results
â”‚
â”œâ”€â”€ Tagalog_Literary_Text.txt     # Literary corpus (~200k words)
â”œâ”€â”€ Tagalog_Religious_Text.txt    # Religious corpus (~90k words)
â”‚
â””â”€â”€ MaBaybay-OCR/                 # OCR system
    â””â”€â”€ Filipino Word Corpus/
        â””â”€â”€ Tagalog_words_74419+.csv
```

## ğŸš€ Quick Start

### Installation

```bash
# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install torch transformers scikit-learn numpy tqdm networkx
```

### Run Evaluation

```bash
# Basic evaluation
python evaluate.py

# With baseline comparisons
python evaluate.py --baselines

# Custom weights
python evaluate.py --weights 0.3 0.4 0.2 0.1
```

### Use in Code

```python
from src import BaybayinDisambiguator

# Initialize
model = BaybayinDisambiguator(
    corpus_files=["Tagalog_Literary_Text.txt", "Tagalog_Religious_Text.txt"]
)

# Disambiguate
candidates = ["ang", ["dito", "rito"], "ay", "maganda"]
result, debug = model.disambiguate(candidates)
print(result)  # ['ang', 'dito', 'ay', 'maganda']
```

## ğŸ“ˆ Methodology

### 1. Data Preparation
- Extracted 500 balanced test sentences
- Generated OCR candidates using MaBaybay OCR simulator
- Distribution: E/I (37.5%), O/U (37.5%), D/R (15%), Combined (10%)

### 2. Feature Engineering
- **Semantic:** RoBERTa embeddings capture contextual meaning
- **Frequency:** Common words in Filipino corpora are favored
- **Co-occurrence:** Bigram statistics model word sequences
- **Morphology:** Filipino affix patterns validate word structure

### 3. Clean Evaluation
- Test sentences excluded from corpus statistics
- Prevents data leakage between train/test
- Results are unbiased and generalizable

## ğŸ“š References

- **RoBERTa Tagalog:** Cruz & Cheng (2020) - `jcblaise/roberta-tagalog-base`
- **bAI-bAI Paper:** Baseline comparison for WE-Only and LLM approaches
- **MaBaybay OCR:** Baybayin character recognition system

## ğŸ“ Citation

```bibtex
@thesis{baybayin_disambiguation_2024,
  title={Context-Aware Baybayin Transliteration Disambiguation},
  author={[Your Name]},
  year={2024}
}
```

## ğŸ“„ License

MIT License
